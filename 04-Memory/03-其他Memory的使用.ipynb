{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. ConversationTokenBufferMemory的使用",
   "id": "9c6f86a87a4e5a64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 举例1:",
   "id": "4fd17588b3d8b035"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T06:43:59.295676Z",
     "start_time": "2025-10-12T06:43:59.254956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain.memory import ConversationTokenBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=os.getenv(\"MODEL\"),\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    ")\n",
    "# NotImplementedError: get_num_tokens_from_messages() is not presently implemented for model doubao-1-5-pro-32k-250115. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.\n",
    "\n",
    "# 使用字符计数替代 token 计数\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=chat_model,\n",
    "    max_token_limit=10,\n",
    ")\n",
    "\n",
    "memory.save_context({\"input\":\"你好吗?\"},{\"output\":\"我很好,谢谢!\"})\n",
    "memory.save_context({\"input\":\"今天天气如何?\"},{\"output\":\"晴天,25度\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "5996a022238f4872",
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model doubao-1-5-pro-32k-250115. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 21\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# 使用字符计数替代 token 计数\u001B[39;00m\n\u001B[0;32m     15\u001B[0m memory \u001B[38;5;241m=\u001B[39m ConversationTokenBufferMemory(\n\u001B[0;32m     16\u001B[0m     llm\u001B[38;5;241m=\u001B[39mchat_model,\n\u001B[0;32m     17\u001B[0m     max_token_limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[0;32m     18\u001B[0m     count_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# 禁用 token 计数\u001B[39;00m\n\u001B[0;32m     19\u001B[0m )\n\u001B[1;32m---> 21\u001B[0m \u001B[43mmemory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m你好吗?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m我很好,谢谢!\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m memory\u001B[38;5;241m.\u001B[39msave_context({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m今天天气如何?\u001B[39m\u001B[38;5;124m\"\u001B[39m},{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m晴天,25度\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(memory\u001B[38;5;241m.\u001B[39mload_memory_variables({}))\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain\\memory\\token_buffer.py:67\u001B[0m, in \u001B[0;36mConversationTokenBufferMemory.save_context\u001B[1;34m(self, inputs, outputs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# Prune buffer if it exceeds max token limit\u001B[39;00m\n\u001B[0;32m     66\u001B[0m buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_memory\u001B[38;5;241m.\u001B[39mmessages\n\u001B[1;32m---> 67\u001B[0m curr_buffer_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_tokens_from_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m curr_buffer_length \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_token_limit:\n\u001B[0;32m     69\u001B[0m     pruned_memory \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1562\u001B[0m, in \u001B[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001B[1;34m(self, messages, tools)\u001B[0m\n\u001B[0;32m   1560\u001B[0m     tokens_per_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1561\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m   1563\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1564\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1565\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for information on how messages are converted to tokens.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1567\u001B[0m     )\n\u001B[0;32m   1568\u001B[0m num_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1569\u001B[0m messages_dict \u001B[38;5;241m=\u001B[39m [_convert_message_to_dict(m) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages]\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: get_num_tokens_from_messages() is not presently implemented for model doubao-1-5-pro-32k-250115. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. ConversationSummaryMemory",
   "id": "655161589e976006"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T06:55:54.512747Z",
     "start_time": "2025-10-12T06:55:48.674032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=os.getenv(\"MODEL\"),\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm = chat_model)\n",
    "\n",
    "memory.save_context({\"input\":\"你好\"},{\"output\":\"怎么了\"})\n",
    "memory.save_context({\"input\":\"你是谁?\"},{\"output\":\"我是AI助手小智\"})\n",
    "memory.save_context({\"input\":\"初次对话,你能介绍一下你自己吗？\"},{\"output\":\"当然可以了,我是一个无所不能的小智\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "6aeed80e040f8021",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': '人类向AI打招呼说“你好”，AI回应询问“怎么了”。随后人类询问AI“你是谁”，AI回答自己是AI助手小智。之后人类因初次对话让AI介绍一下自己，AI表示当然可以，并称自己是无所不能的小智。 '}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 举例2: 如果实例化ConversationSummaryMemory前,已经有历史消息,可以调用from_messages()实例化",
   "id": "8cf9d1ade1c94051"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T07:03:22.814153Z",
     "start_time": "2025-10-12T07:03:18.626897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "from langchain.memory import ConversationSummaryMemory,ChatMessageHistory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=os.getenv(\"MODEL\"),\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    ")\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"你好，你是谁？\")\n",
    "history.add_ai_message(\"我是AI助手小智\")\n",
    "\n",
    "memory = ConversationSummaryMemory.from_messages(\n",
    "    llm = chat_model,\n",
    "    #是生成摘要的原材料 保留完整对话供必要时回溯。当新增对话时，LLM需要结合原始历史生成新摘要\n",
    "    chat_memory = history,\n",
    ")\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "memory.save_context(inputs={\"human\":\"我的名字叫小明\"},outputs={\"ai\":\"很高兴认识你\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "# 记录了历史的交互的消息\n",
    "print(memory.chat_memory.messages)"
   ],
   "id": "ff5158817a28f0dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': '人类询问“你好，你是谁？”，AI回答“我是AI助手小智”。'}\n",
      "{'history': '人类询问“你好，你是谁？”，AI回答“我是AI助手小智”。随后人类告知AI自己的名字叫小明，AI表示很高兴认识人类。 '}\n",
      "[HumanMessage(content='你好，你是谁？', additional_kwargs={}, response_metadata={}), AIMessage(content='我是AI助手小智', additional_kwargs={}, response_metadata={}), HumanMessage(content='我的名字叫小明', additional_kwargs={}, response_metadata={}), AIMessage(content='很高兴认识你', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. ConversationSummaryBufferMemory",
   "id": "5ce4627ff51575c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 举例1:",
   "id": "4e3617ba21bd838a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T07:40:23.664872Z",
     "start_time": "2025-10-12T07:40:23.615957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import dotenv\n",
    "import os\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=os.getenv(\"MODEL\"),\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    ")\n",
    "\n",
    "memory =  ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit=40, # 控制缓冲区的大小\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# NotImplementedError: get_num_tokens_from_messages() is not presently implemented for model doubao-1-5-pro-32k-250115. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.\n",
    "\n",
    "# 向memory中存储信息\n",
    "memory.save_context(inputs={\"input\":\"你好,我的名字叫小明\"},outputs={\"output\":\"很高兴认识你\"})\n",
    "memory.save_context({\"input\": \"李白是哪个朝代的诗人\"}, {\"output\": \"李白是唐朝诗人\"})\n",
    "memory.save_context({\"input\": \"唐宋八大家里有苏轼吗？\"}, {\"output\": \"有\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(memory.chat_memory.messages)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "e927f2d4d5d41f8c",
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model doubao-1-5-pro-32k-250115. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 22\u001B[0m\n\u001B[0;32m     15\u001B[0m memory \u001B[38;5;241m=\u001B[39m  ConversationSummaryBufferMemory(\n\u001B[0;32m     16\u001B[0m     llm \u001B[38;5;241m=\u001B[39m llm,\n\u001B[0;32m     17\u001B[0m     max_token_limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m40\u001B[39m, \u001B[38;5;66;03m# 控制缓冲区的大小\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     return_messages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     19\u001B[0m )\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# 向memory中存储信息\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m \u001B[43mmemory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m你好,我的名字叫小明\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m很高兴认识你\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m memory\u001B[38;5;241m.\u001B[39msave_context({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m李白是哪个朝代的诗人\u001B[39m\u001B[38;5;124m\"\u001B[39m}, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m李白是唐朝诗人\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[0;32m     24\u001B[0m memory\u001B[38;5;241m.\u001B[39msave_context({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m唐宋八大家里有苏轼吗？\u001B[39m\u001B[38;5;124m\"\u001B[39m}, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m有\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain\\memory\\summary_buffer.py:101\u001B[0m, in \u001B[0;36mConversationSummaryBufferMemory.save_context\u001B[1;34m(self, inputs, outputs)\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001B[39;00m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msave_context(inputs, outputs)\n\u001B[1;32m--> 101\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprune\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain\\memory\\summary_buffer.py:115\u001B[0m, in \u001B[0;36mConversationSummaryBufferMemory.prune\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001B[39;00m\n\u001B[0;32m    114\u001B[0m buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_memory\u001B[38;5;241m.\u001B[39mmessages\n\u001B[1;32m--> 115\u001B[0m curr_buffer_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_tokens_from_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m curr_buffer_length \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_token_limit:\n\u001B[0;32m    117\u001B[0m     pruned_memory \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1562\u001B[0m, in \u001B[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001B[1;34m(self, messages, tools)\u001B[0m\n\u001B[0;32m   1560\u001B[0m     tokens_per_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1561\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m   1563\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1564\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1565\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for information on how messages are converted to tokens.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1567\u001B[0m     )\n\u001B[0;32m   1568\u001B[0m num_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1569\u001B[0m messages_dict \u001B[38;5;241m=\u001B[39m [_convert_message_to_dict(m) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages]\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: get_num_tokens_from_messages() is not presently implemented for model doubao-1-5-pro-32k-250115. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 举例2: 模拟客服交互",
   "id": "7e3287eca1cd3650"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T07:50:48.057340Z",
     "start_time": "2025-10-12T07:50:46.726097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.llm import LLMChain\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# 1. 初始化大语言模型（修正模型名称）\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=os.getenv(\"MODEL\"),\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    ")\n",
    "\n",
    "# 2. 定义提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是电商客服助手，用中文友好回复用户问题。保持专业但亲切的语气。\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 3. 创建带摘要缓冲的记忆系统\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=400,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 4. 创建对话链\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# 5. 模拟多轮对话\n",
    "dialogue = [\n",
    "    (\"你好，我想查询订单12345的状态\", None),\n",
    "    (\"这个订单是上周五下的\", None),\n",
    "    (\"我现在急着用，能加急处理吗\", None),\n",
    "    (\"等等，我可能记错订单号了，应该是12346\", None),\n",
    "    (\"对了，你们退货政策是怎样的\", None)\n",
    "]\n",
    "\n",
    "# 6. 执行对话\n",
    "print(\"=== 对话开始 ===\")\n",
    "for user_input, _ in dialogue:\n",
    "    response = chain.invoke({\"input\": user_input})\n",
    "    print(f\"用户: {user_input}\")\n",
    "    print(f\"客服: {response['text']}\\n\")\n",
    "\n",
    "# 7. 查看当前记忆状态\n",
    "print(\"\\n=== 当前记忆内容 ===\")\n",
    "memory_content = memory.load_memory_variables({})\n",
    "print(memory_content['chat_history'])"
   ],
   "id": "1ee96d79a3737473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 对话开始 ===\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model doubao-1-5-pro-32k-250115. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 51\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=== 对话开始 ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m user_input, _ \u001B[38;5;129;01min\u001B[39;00m dialogue:\n\u001B[1;32m---> 51\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_input\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m用户: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muser_input\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m客服: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain\\chains\\base.py:170\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[0;32m    164\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    165\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    166\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    167\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    168\u001B[0m     )\n\u001B[1;32m--> 170\u001B[0m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprep_outputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    176\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain\\chains\\base.py:487\u001B[0m, in \u001B[0;36mChain.prep_outputs\u001B[1;34m(self, inputs, outputs, return_only_outputs)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_outputs(outputs)\n\u001B[0;32m    486\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 487\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmemory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_only_outputs:\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain\\memory\\summary_buffer.py:101\u001B[0m, in \u001B[0;36mConversationSummaryBufferMemory.save_context\u001B[1;34m(self, inputs, outputs)\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001B[39;00m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msave_context(inputs, outputs)\n\u001B[1;32m--> 101\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprune\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain\\memory\\summary_buffer.py:115\u001B[0m, in \u001B[0;36mConversationSummaryBufferMemory.prune\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001B[39;00m\n\u001B[0;32m    114\u001B[0m buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_memory\u001B[38;5;241m.\u001B[39mmessages\n\u001B[1;32m--> 115\u001B[0m curr_buffer_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_tokens_from_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m curr_buffer_length \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_token_limit:\n\u001B[0;32m    117\u001B[0m     pruned_memory \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1562\u001B[0m, in \u001B[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001B[1;34m(self, messages, tools)\u001B[0m\n\u001B[0;32m   1560\u001B[0m     tokens_per_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1561\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m   1563\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1564\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1565\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for information on how messages are converted to tokens.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1567\u001B[0m     )\n\u001B[0;32m   1568\u001B[0m num_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1569\u001B[0m messages_dict \u001B[38;5;241m=\u001B[39m [_convert_message_to_dict(m) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages]\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: get_num_tokens_from_messages() is not presently implemented for model doubao-1-5-pro-32k-250115. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. ConversationEntityMemory(了解)",
   "id": "18a78be402b72da5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T09:02:22.382461Z",
     "start_time": "2025-10-12T09:02:00.833567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# 1. 初始化大语言模型（修正模型名称）\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=os.getenv(\"MODEL\"),\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    ")\n",
    "# 2. 初始化实体记忆\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "\n",
    "# 3. 创建对话链\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=memory,  # 使用上面创建的memory对象\n",
    "    verbose=True,  # 开启详细日志\n",
    ")\n",
    "\n",
    "# 4. 进行多轮对话\n",
    "print(\"=== 对话开始 ===\")\n",
    "response1 = chain.invoke(input=\"你好，我叫蜘蛛侠。我的好朋友包括钢铁侠、美国队长和绿巨人。\")\n",
    "print(f\"AI回复: {response1['text']}\")\n",
    "\n",
    "response2 = chain.invoke(input=\"我住在纽约。\")\n",
    "print(f\"AI回复: {response2['text']}\")\n",
    "\n",
    "response3 = chain.invoke(input=\"我使用的装备是由斯塔克工业提供的。\")\n",
    "print(f\"AI回复: {response3['text']}\")\n",
    "\n",
    "# 5. 查询存储的实体信息\n",
    "print(\"\\n=== 当前存储的实体信息 ===\")\n",
    "print(memory.entity_store.store)\n",
    "\n",
    "# 6. 基于记忆提问\n",
    "print(\"\\n=== 基于记忆的提问 ===\")\n",
    "final_answer = chain.invoke(input=\"你能告诉我蜘蛛侠住在哪里以及他的好朋友有哪些吗？\")\n",
    "print(f\"AI回答: {final_answer['text']}\")"
   ],
   "id": "3c08b649a35acd97",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_39092\\2072033208.py:17: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationEntityMemory(llm=llm)\n",
      "C:\\Users\\Admin\\.conda\\envs\\py310\\lib\\site-packages\\pydantic\\main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 对话开始 ===\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'蜘蛛侠': '', '钢铁侠': '', '美国队长': '', '绿巨人': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: 你好，我叫蜘蛛侠。我的好朋友包括钢铁侠、美国队长和绿巨人。\n",
      "You:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "AI回复: 你好呀，蜘蛛侠！能和钢铁侠、美国队长还有绿巨人成为好朋友，你们的组合一定能守护世界的和平，我很好奇你们一起并肩作战的时候是不是特别精彩，能和我分享一次你们印象深刻的战斗经历吗？ \n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'纽约': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好，我叫蜘蛛侠。我的好朋友包括钢铁侠、美国队长和绿巨人。\n",
      "AI: 你好呀，蜘蛛侠！能和钢铁侠、美国队长还有绿巨人成为好朋友，你们的组合一定能守护世界的和平，我很好奇你们一起并肩作战的时候是不是特别精彩，能和我分享一次你们印象深刻的战斗经历吗？ \n",
      "Last line:\n",
      "Human: 我住在纽约。\n",
      "You:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "AI回复: 纽约可是一座繁华又充满活力的城市呢！那里高楼林立，充满了无限的可能性。你是不是经常在纽约的大街小巷飞来飞去，守护着这座城市的安全呀？纽约有没有哪个地方是你特别喜欢去的呢？ \n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'斯塔克工业': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好，我叫蜘蛛侠。我的好朋友包括钢铁侠、美国队长和绿巨人。\n",
      "AI: 你好呀，蜘蛛侠！能和钢铁侠、美国队长还有绿巨人成为好朋友，你们的组合一定能守护世界的和平，我很好奇你们一起并肩作战的时候是不是特别精彩，能和我分享一次你们印象深刻的战斗经历吗？ \n",
      "Human: 我住在纽约。\n",
      "AI: 纽约可是一座繁华又充满活力的城市呢！那里高楼林立，充满了无限的可能性。你是不是经常在纽约的大街小巷飞来飞去，守护着这座城市的安全呀？纽约有没有哪个地方是你特别喜欢去的呢？ \n",
      "Last line:\n",
      "Human: 我使用的装备是由斯塔克工业提供的。\n",
      "You:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "AI回复: 斯塔克工业可是非常厉害的科技企业呢！他们的科技水平领先，由他们提供的装备肯定十分先进。你的装备一定有很多强大的功能，能在你守护纽约的时候帮上大忙。能和我说说这些装备都有哪些特别之处吗？ \n",
      "\n",
      "=== 当前存储的实体信息 ===\n",
      "{'蜘蛛侠': '蜘蛛侠名叫蜘蛛侠，他的好朋友包括钢铁侠、美国队长和绿巨人。', '钢铁侠': '钢铁侠是蜘蛛侠的好朋友。', '美国队长': '美国队长是蜘蛛侠的好朋友。', '绿巨人': '绿巨人是蜘蛛侠的好朋友。', '纽约': '蜘蛛侠住在纽约。', '斯塔克工业': '斯塔克工业为蜘蛛侠提供他所使用的装备。'}\n",
      "\n",
      "=== 基于记忆的提问 ===\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'蜘蛛侠': '蜘蛛侠名叫蜘蛛侠，他的好朋友包括钢铁侠、美国队长和绿巨人。'}\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好，我叫蜘蛛侠。我的好朋友包括钢铁侠、美国队长和绿巨人。\n",
      "AI: 你好呀，蜘蛛侠！能和钢铁侠、美国队长还有绿巨人成为好朋友，你们的组合一定能守护世界的和平，我很好奇你们一起并肩作战的时候是不是特别精彩，能和我分享一次你们印象深刻的战斗经历吗？ \n",
      "Human: 我住在纽约。\n",
      "AI: 纽约可是一座繁华又充满活力的城市呢！那里高楼林立，充满了无限的可能性。你是不是经常在纽约的大街小巷飞来飞去，守护着这座城市的安全呀？纽约有没有哪个地方是你特别喜欢去的呢？ \n",
      "Human: 我使用的装备是由斯塔克工业提供的。\n",
      "AI: 斯塔克工业可是非常厉害的科技企业呢！他们的科技水平领先，由他们提供的装备肯定十分先进。你的装备一定有很多强大的功能，能在你守护纽约的时候帮上大忙。能和我说说这些装备都有哪些特别之处吗？ \n",
      "Last line:\n",
      "Human: 你能告诉我蜘蛛侠住在哪里以及他的好朋友有哪些吗？\n",
      "You:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "AI回答: 蜘蛛侠住在纽约，他的好朋友包括钢铁侠、美国队长和绿巨人。 \n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. ConversationKGMemory(了解)",
   "id": "54303dfb12ef8302"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "pip install networkx\n",
    "```"
   ],
   "id": "3ee56e17289a42ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T09:07:25.722966Z",
     "start_time": "2025-10-12T09:07:21.954263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 导入相关包\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# 1. 初始化大语言模型（修正模型名称）\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=os.getenv(\"MODEL\"),\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    ")\n",
    "# 3. 定义ConversationKGMemory对象\n",
    "memory = ConversationKGMemory(llm=llm)\n",
    "\n",
    "# 4. 保存会话\n",
    "memory.save_context({\"input\": \"向山姆问好\"}, {\"output\": \"山姆是谁\"})\n",
    "memory.save_context({\"input\": \"山姆是我的朋友\"}, {\"output\": \"好的\"})\n",
    "\n",
    "# 5. 查询会话\n",
    "memory_data = memory.load_memory_variables({\"input\": \"山姆是谁\"})\n",
    "print(\"=== 知识图谱记忆内容 ===\")\n",
    "print(memory_data)"
   ],
   "id": "9376fefca31c8f47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 知识图谱记忆内容 ===\n",
      "{'history': 'On 山姆: 山姆 是 我的朋友.'}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T09:07:28.636258Z",
     "start_time": "2025-10-12T09:07:27.464446Z"
    }
   },
   "cell_type": "code",
   "source": "memory.get_knowledge_triplets(\"她最喜欢的颜色是红色\")",
   "id": "afe79b2522087817",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KnowledgeTriple(subject='她', predicate='最喜欢的颜色是', object_='红色')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. VectorStoreRetrieverMemory(了解)",
   "id": "7cffd20665d74133"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T09:57:47.904244Z",
     "start_time": "2025-10-12T09:57:47.560712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.memory import VectorStoreRetrieverMemory, ConversationBufferMemory\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "# 1. 定义ConversationBufferMemory对象并保存上下文\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"我最喜欢的食物是披萨\"}, {\"output\": \"很高兴知道\"})\n",
    "memory.save_context({\"input\": \"我喜欢的运动是跑步\"}, {\"output\": \"好的,我知道了\"})\n",
    "memory.save_context({\"input\": \"我最喜欢的运动是足球\"}, {\"output\": \"好的,我知道了\"})\n",
    "\n",
    "# 2. 定义向量嵌入模型\n",
    "# BadRequestError: Error code: 400 - {'error': {'code': 'InvalidParameter', 'message': 'The parameter `input[0]` specified in the request are not valid: expected a string, but got `[35075 25 50534 239 32335 83601 250 25340 95 9554 72406 253 53953 21043 24326 104 81180 101]` instead. Request id: 02176026306822061ee2447fb8673108fd2fdb6d547618f607175', 'param': 'input[0]', 'type': 'BadRequest'}}\n",
    "embeddings_model = OpenAIEmbeddings(model=os.getenv(\"MODEL_EMBEDDING\"),\n",
    "                                    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    "                                    base_url=os.getenv(\"BASE_URL\"),)\n",
    "\n",
    "# 3. 初始化向量数据库\n",
    "# 提取对话历史并转换为文本列表\n",
    "conversation_history = memory.buffer.split(\"\\n\")\n",
    "vectorstore = FAISS.from_texts(conversation_history, embeddings_model)\n",
    "\n",
    "# 4. 定义检索对象\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "\n",
    "# 5. 初始化VectorStoreRetrieverMemory\n",
    "vector_memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# 6. 查询记忆\n",
    "result = vector_memory.load_memory_variables({\"prompt\": \"我最喜欢的食物是\"})\n",
    "print(\"\\n=== 检索结果 ===\")\n",
    "print(result)"
   ],
   "id": "84983b0b57ae26fa",
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'InvalidParameter', 'message': 'The parameter `input[0]` specified in the request are not valid: expected a string, but got `[35075 25 50534 239 32335 83601 250 25340 95 9554 72406 253 53953 21043 24326 104 81180 101]` instead. Request id: 02176026306822061ee2447fb8673108fd2fdb6d547618f607175', 'param': 'input[0]', 'type': 'BadRequest'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 25\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# 3. 初始化向量数据库\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# 提取对话历史并转换为文本列表\u001B[39;00m\n\u001B[0;32m     24\u001B[0m conversation_history \u001B[38;5;241m=\u001B[39m memory\u001B[38;5;241m.\u001B[39mbuffer\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 25\u001B[0m vectorstore \u001B[38;5;241m=\u001B[39m \u001B[43mFAISS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconversation_history\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membeddings_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# 4. 定义检索对象\u001B[39;00m\n\u001B[0;32m     28\u001B[0m retriever \u001B[38;5;241m=\u001B[39m vectorstore\u001B[38;5;241m.\u001B[39mas_retriever(search_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001B[0m, in \u001B[0;36mFAISS.from_texts\u001B[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001B[0m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m   1017\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfrom_texts\u001B[39m(\n\u001B[0;32m   1018\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1023\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m   1024\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m FAISS:\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001B[39;00m\n\u001B[0;32m   1026\u001B[0m \n\u001B[0;32m   1027\u001B[0m \u001B[38;5;124;03m    This is a user friendly interface that:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001B[39;00m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1043\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[43membedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__from(\n\u001B[0;32m   1045\u001B[0m         texts,\n\u001B[0;32m   1046\u001B[0m         embeddings,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1050\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1051\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:591\u001B[0m, in \u001B[0;36mOpenAIEmbeddings.embed_documents\u001B[1;34m(self, texts, chunk_size, **kwargs)\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[0;32m    590\u001B[0m engine \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeployment)\n\u001B[1;32m--> 591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_len_safe_embeddings(\n\u001B[0;32m    592\u001B[0m     texts, engine\u001B[38;5;241m=\u001B[39mengine, chunk_size\u001B[38;5;241m=\u001B[39mchunk_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    593\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:479\u001B[0m, in \u001B[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[1;34m(self, texts, engine, chunk_size, **kwargs)\u001B[0m\n\u001B[0;32m    477\u001B[0m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[1;32m--> 479\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[0;32m    480\u001B[0m         \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mtokens[i : i \u001B[38;5;241m+\u001B[39m _chunk_size], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mclient_kwargs\n\u001B[0;32m    481\u001B[0m     )\n\u001B[0;32m    482\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    483\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mmodel_dump()\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\openai\\resources\\embeddings.py:132\u001B[0m, in \u001B[0;36mEmbeddings.create\u001B[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    126\u001B[0m             embedding\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[0;32m    127\u001B[0m                 base64\u001B[38;5;241m.\u001B[39mb64decode(data), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    128\u001B[0m             )\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[1;32m--> 132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/embeddings\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\openai\\_base_client.py:1259\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1246\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1247\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1254\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1255\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1256\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1257\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1258\u001B[0m     )\n\u001B[1;32m-> 1259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\openai\\_base_client.py:1047\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1044\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1046\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1047\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mBadRequestError\u001B[0m: Error code: 400 - {'error': {'code': 'InvalidParameter', 'message': 'The parameter `input[0]` specified in the request are not valid: expected a string, but got `[35075 25 50534 239 32335 83601 250 25340 95 9554 72406 253 53953 21043 24326 104 81180 101]` instead. Request id: 02176026306822061ee2447fb8673108fd2fdb6d547618f607175', 'param': 'input[0]', 'type': 'BadRequest'}}"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
